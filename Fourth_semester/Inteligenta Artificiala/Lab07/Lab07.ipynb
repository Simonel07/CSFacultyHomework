{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "class BGDRegression:\n",
    "    def __init__(self):\n",
    "        self.intercept_ = 0.0\n",
    "        self.coefficient_ = []\n",
    "\n",
    "    def fit(self, x, y, learning_rate=0.001, no_epochs=1000):\n",
    "        self.coefficient_ = [random.random() for _ in range(len(x[0]) + 1)]\n",
    "        for epoch in range(no_epochs):\n",
    "            errors = []\n",
    "            for i in range(len(x)):\n",
    "                y_computed = self.eval(x[i])\n",
    "                errors.append(y_computed - y[i])\n",
    "            error = mean(errors)\n",
    "            for i in range(len(x)):\n",
    "                for j in range(0, len(x[0])):\n",
    "                    self.coefficient_[j] = self.coefficient_[j] - learning_rate * error * x[i][j]\n",
    "                self.coefficient_[len(x[0])] = self.coefficient_[len(x[0])] - learning_rate * error * 1\n",
    "        self.intercept_ = self.coefficient_[-1]\n",
    "        self.coefficient_ = self.coefficient_[:-1]\n",
    "\n",
    "    def eval(self, xi):\n",
    "        yi = self.coefficient_[-1]\n",
    "        for j in range(len(xi)):\n",
    "            yi += self.coefficient_[j] * xi[j]\n",
    "        return yi\n",
    "\n",
    "    def predict(self, x):\n",
    "        y_computed = [self.eval(xi) for xi in x]\n",
    "        return y_computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDRegression:\n",
    "    def __init__(self):\n",
    "        self.intercept_ = 0.0\n",
    "        self.coefficient_ = []\n",
    "\n",
    "    def fit(self, x, y, learning_rate=0.001, no_epochs=1000):\n",
    "        self.coefficient_ = [random.random() for _ in range(len(x[0]) + 1)]\n",
    "        for epoch in range(no_epochs):\n",
    "            for i in range(len(x)):\n",
    "                y_computed = self.eval(x[i])\n",
    "                crt_error = y_computed - y[i]\n",
    "                for j in range(0, len(x[0])):\n",
    "                    self.coefficient_[j] = self.coefficient_[j] - learning_rate * crt_error * x[i][j]\n",
    "                self.coefficient_[len(x[0])] = self.coefficient_[len(x[0])] - learning_rate * crt_error * 1\n",
    "        self.intercept_ = self.coefficient_[-1]\n",
    "        self.coefficient_ = self.coefficient_[:-1]\n",
    "\n",
    "    def eval(self, xi):\n",
    "        yi = self.coefficient_[-1]\n",
    "        for j in range(len(xi)):\n",
    "            yi += self.coefficient_[j] * xi[j]\n",
    "        return yi\n",
    "\n",
    "    def predict(self, x):\n",
    "        y_computed = [self.eval(xi) for xi in x]\n",
    "        return y_computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, input_features, output_feature):\n",
    "    file = pd.read_csv(filename)\n",
    "    features = []\n",
    "    for feature in input_features:\n",
    "        features.append([float(value) for value in file[feature]])\n",
    "    output_feature = [float(value) for value in file[output_feature]]\n",
    "    return features, output_feature\n",
    "\n",
    "\n",
    "def plot_histogram(x, variable_name):\n",
    "    plt.hist(x, 10)\n",
    "    plt.title('Histogram of ' + variable_name)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_linearity(input_feature, output_feature, input_name, output_name):\n",
    "    plt.plot(input_feature, output_feature, 'ro')\n",
    "    plt.xlabel(input_name)\n",
    "    plt.ylabel(output_name)\n",
    "    plt.title(input_name + ' vs. ' + output_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(features, result):\n",
    "    np.random.seed(5)\n",
    "    indexes = [i for i in range(len(result))]\n",
    "    train_sample_indexes = np.random.choice(indexes, int(0.8 * len(result)), replace=False)\n",
    "    validation_sample_indexes = [i for i in range(len(result)) if i not in train_sample_indexes]\n",
    "    train_features = []\n",
    "    validation_features = []\n",
    "    for feature in features:\n",
    "        train_features.append([feature[i] for i in train_sample_indexes])\n",
    "        validation_features.append([feature[i] for i in validation_sample_indexes])\n",
    "    train_result = [result[i] for i in train_sample_indexes]\n",
    "    validation_result = [result[i] for i in validation_sample_indexes]\n",
    "    return train_features, train_result, validation_features, validation_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_and_test(train_features, train_result, validation_features, validation_result, input_name, output_name):\n",
    "    plt.plot(train_features, train_result, 'ro', label='Training data')\n",
    "    plt.plot(validation_features, validation_result, 'g^', label='Testing data')\n",
    "    plt.xlabel(input_name)\n",
    "    plt.ylabel(output_name)\n",
    "    plt.title('Train and test')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_tool(train_input, train_output, validation_input):\n",
    "    xx = [[el] for el in train_input]\n",
    "    regressor = linear_model.SGDRegressor(alpha=0.01, max_iter=1000)\n",
    "    regressor.fit(xx, train_output)\n",
    "    w0, w1 = regressor.intercept_[0], regressor.coef_[0]\n",
    "    print('Learnt model by tool: f(x) =', w0, '+', w1, '* x')\n",
    "    computed_outputs = regressor.predict([[x] for x in validation_input])\n",
    "    return w0, w1, computed_outputs\n",
    "\n",
    "\n",
    "def learning_by_me(train_input, train_output, validation_input):\n",
    "    xx = [[el] for el in train_input]\n",
    "    regressor = BGDRegression()\n",
    "    regressor.fit(xx, train_output)\n",
    "    w0, w1 = regressor.intercept_, regressor.coefficient_[0]\n",
    "    print('Learnt model by me: f(x) = ', w0, '+', w1, '* x')\n",
    "    computed_outputs = regressor.predict([[x] for x in validation_input])\n",
    "    return w0, w1, computed_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error(computed_output, validation_output, label):\n",
    "    error = mean_squared_error(validation_output, computed_output)\n",
    "    print(f'Prediction error by {label}:  ', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_me_multi_variant(train_inputs, train_output, validation_inputs):\n",
    "    xx = [[el1, el2] for el1, el2 in zip(*train_inputs)]\n",
    "    regressor = BGDRegression()\n",
    "    regressor.fit(xx, train_output)\n",
    "    w0, w1, w2 = regressor.intercept_, regressor.coefficient_[0], regressor.coefficient_[1]\n",
    "    print('Learnt model by me: f(x) = ', w0, '+', w1, '* x1', '+', w2, '* x2')\n",
    "    computed_outputs = regressor.predict([[x, y] for x, y in zip(*validation_inputs)])\n",
    "    return w0, w1, w2, computed_outputs\n",
    "\n",
    "\n",
    "def learning_by_tool_multi_variant(train_inputs, train_output, validation_inputs):\n",
    "    xx = [[el1, el2] for el1, el2 in zip(*train_inputs)]\n",
    "    regressor = linear_model.SGDRegressor(alpha=0.01, max_iter=1000)\n",
    "    regressor.fit(xx, train_output)\n",
    "    w0, w1, w2 = regressor.intercept_[0], regressor.coef_[0], regressor.coef_[1]\n",
    "    print('Learnt model by tool: f(x) =', w0, '+', w1, '* x1', '+', w2, '* x2')\n",
    "    computed_outputs = regressor.predict([[x, y] for x, y in zip(*validation_inputs)])\n",
    "    return w0, w1, w2, computed_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(features, mean_value=None, std_dev=None):  # statistical normalisation\n",
    "    if mean_value is None:\n",
    "        mean_value = sum(features) / len(features)\n",
    "    if std_dev is None:\n",
    "        std_dev = (1 / len(features) * sum([(feat - mean_value) ** 2 for feat in features])) ** 0.5\n",
    "    normalised_features = [(feat - mean_value) / std_dev for feat in features]\n",
    "    return normalised_features, mean_value, std_dev\n",
    "\n",
    "\n",
    "def normalise_data(data):\n",
    "    mean_value, std_dev = None, None\n",
    "    normalised_data = []\n",
    "    for dat in data:\n",
    "        normalised_features, mean_value, std_dev = normalise(dat, mean_value, std_dev)\n",
    "        normalised_data.append(normalised_features)\n",
    "    return normalised_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_multi_output(filename, input_features, output_feature):\n",
    "    file = pd.read_csv(filename)\n",
    "    features = []\n",
    "    outputs = []\n",
    "    for feature in input_features:\n",
    "        features.append([float(value) for value in file[feature]])\n",
    "    for output in output_feature:\n",
    "        outputs.append([float(value) for value in file[output]])\n",
    "    return features, outputs\n",
    "\n",
    "\n",
    "def train_and_test_multi_output(features, results):\n",
    "    np.random.seed(5)\n",
    "    indexes = [i for i in range(len(results[0]))]\n",
    "    train_sample_indexes = np.random.choice(indexes, int(0.8 * len(results[0])), replace=False)\n",
    "    validation_sample_indexes = [i for i in range(len(results[0])) if i not in train_sample_indexes]\n",
    "    train_features = []\n",
    "    validation_features = []\n",
    "    train_outputs = []\n",
    "    validation_outputs = []\n",
    "    for feature in features:\n",
    "        train_features.append([feature[i] for i in train_sample_indexes])\n",
    "        validation_features.append([feature[i] for i in validation_sample_indexes])\n",
    "    for result in results:\n",
    "        train_outputs.append([result[i] for i in train_sample_indexes])\n",
    "        validation_outputs.append([result[i] for i in validation_sample_indexes])\n",
    "    return train_features, train_outputs, validation_features, validation_outputs\n",
    "\n",
    "\n",
    "def learning_by_me_multi_outputs(train_inputs, train_output, validation_inputs):\n",
    "    xx = [[el1, el2] for el1, el2 in zip(*train_inputs)]\n",
    "    regressor = BGDRegression()\n",
    "    results = []\n",
    "    computed_outputs = []\n",
    "    for i in train_output:\n",
    "        regressor.fit(xx, i)\n",
    "        w0, w1, w2 = regressor.intercept_, regressor.coefficient_[0], regressor.coefficient_[1]\n",
    "        results.append([w0, w1, w2])\n",
    "        print('Learnt model by me: f(x) = ', w0, '+', w1, '* x1', '+', w2, '* x2')\n",
    "        computed_outputs.append(regressor.predict([[x, y] for x, y in zip(*validation_inputs)]))\n",
    "    return results, computed_outputs\n",
    "\n",
    "\n",
    "def learning_by_me_multi_dependent_outputs():\n",
    "    input_for_multiple_output, output_for_multiple_output = make_regression(n_samples=1000, n_features=2, n_targets=2,\n",
    "                                                                            random_state=1, noise=0.5)\n",
    "    np.random.seed(5)\n",
    "    train_indexes = np.random.choice([i for i in range(len(input_for_multiple_output))],\n",
    "                                     int(0.8 * len(input_for_multiple_output)), replace=False)\n",
    "    validation_indexes = [i for i in range(len(input_for_multiple_output)) if i not in train_indexes]\n",
    "    train_input = [input_for_multiple_output[i] for i in train_indexes]\n",
    "    train_output = [output_for_multiple_output[i] for i in train_indexes]\n",
    "    validation_input = [input_for_multiple_output[i] for i in validation_indexes]\n",
    "    validation_output = [output_for_multiple_output[i] for i in validation_indexes]\n",
    "    model = linear_model.LinearRegression()\n",
    "    model.fit(train_input, train_output)\n",
    "    plt.plot(*list(map(list, zip(*validation_output))), 'ro')\n",
    "    plt.plot(*list(map(list, zip(*model.predict(validation_input)))), 'g*')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.show()\n",
    "    for i in range(len(model.intercept_)):\n",
    "        print('Learnt model by me: f(x) = ', model.intercept_[i], '+', model.coef_[i][0], '* x1', '+', model.coef_[i][1]\n",
    "              , '* x2')\n",
    "    return model.intercept_, model.coef_, model.predict(validation_input), validation_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_v1 = 'data/world-happiness-report-2017.csv'\n",
    "inputs, output = load_data(file_v1, ['Economy..GDP.per.Capita.', 'Freedom'],\n",
    "                           'Happiness.Score')\n",
    "train_inputs, train_outputs, validation_inputs, validation_outputs = train_and_test(inputs, output)\n",
    "plot_histogram(inputs[0], \"capita GDP\")\n",
    "plot_histogram(output, \"happiness score\")\n",
    "plot_linearity(inputs[0], output, \"capita GDP\", \"happiness\")\n",
    "plot_train_and_test(train_inputs[0], train_outputs, validation_inputs[0], validation_outputs, \"capita GDP\",\n",
    "                    \"happiness\")\n",
    "print(\"One output, one feature:\")\n",
    "tool_result = learning_by_tool(train_inputs[0], train_outputs, validation_inputs[0])\n",
    "my_result = learning_by_me(train_inputs[0], train_outputs, validation_inputs[0])\n",
    "calculate_error(my_result[2], validation_outputs, 'me')\n",
    "print(\"One output, two features:\")\n",
    "tool_result_multi = learning_by_tool_multi_variant(train_inputs, train_outputs, validation_inputs)\n",
    "my_result_multi = learning_by_me_multi_variant(train_inputs, train_outputs, validation_inputs)\n",
    "calculate_error(my_result_multi[3], validation_outputs, 'me')\n",
    "\n",
    "inputs, output = load_data_multi_output(file_v1, ['Economy..GDP.per.Capita.', 'Freedom'],\n",
    "                                        ['Happiness.Score', 'Generosity'])\n",
    "data = normalise_data([*inputs, *output])\n",
    "inputs, output = data[:2], data[:-1]\n",
    "train_inputs, train_outputs, validation_inputs, validation_outputs = train_and_test_multi_output(inputs, output)\n",
    "print(\"Two independent outputs, two features:\")\n",
    "my_result_multi_outputs = learning_by_me_multi_outputs(train_inputs, train_outputs, validation_inputs)\n",
    "calculate_error(my_result_multi_outputs[-1][0], validation_outputs[0], 'me')\n",
    "calculate_error(my_result_multi_outputs[-1][1], validation_outputs[1], 'me')\n",
    "print(\"Two dependent outputs, two features:\")\n",
    "my_result_multi_outputs = learning_by_me_multi_dependent_outputs()\n",
    "calculate_error(my_result_multi_outputs[-1], my_result_multi_outputs[-2], 'me')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
